{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a002cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb59d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 542 unique ERC-20 tokens in pools created feb 2024.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "UNISWAP_V3_SUBGRAPH_URL = \"https://gateway.thegraph.com/api/214ad8fea4f34456c35952f32963b967/subgraphs/id/5zvR82QoaXYFyDEKLZ9t6v9adgnptxYpKpSbxtgVENFV\"\n",
    "\n",
    "def fetch_october_2023_tokens_from_pools():\n",
    "    query = \"\"\"\n",
    "    {\n",
    "      pools(\n",
    "        first: 1000,\n",
    "        orderBy: createdAtTimestamp,\n",
    "        orderDirection: asc,\n",
    "        where: {\n",
    "          createdAtTimestamp_gte: 1709164800,\n",
    "          createdAtTimestamp_lt:  1711843200\n",
    "        }\n",
    "      ) {\n",
    "        id\n",
    "        createdAtTimestamp\n",
    "        token0 {\n",
    "          id\n",
    "          symbol\n",
    "          name\n",
    "          decimals\n",
    "        }\n",
    "        token1 {\n",
    "          id\n",
    "          symbol\n",
    "          name\n",
    "          decimals\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(UNISWAP_V3_SUBGRAPH_URL, json={\"query\": query})\n",
    "    data = response.json()\n",
    "\n",
    "    if \"errors\" in data:\n",
    "        print(\"GraphQL error:\", data[\"errors\"])\n",
    "        return []\n",
    "\n",
    "    tokens = {}\n",
    "    for pool in data[\"data\"][\"pools\"]:\n",
    "        for token_key in [\"token0\", \"token1\"]:\n",
    "            token = pool[token_key]\n",
    "            tokens[token[\"id\"]] = {\n",
    "                \"address\": token[\"id\"],\n",
    "                \"symbol\": token[\"symbol\"],\n",
    "                \"name\": token[\"name\"],\n",
    "                \"decimals\": int(token[\"decimals\"])\n",
    "            }\n",
    "\n",
    "    print(f\"✅ Found {len(tokens)} unique ERC-20 tokens in pools created feb 2024.\")\n",
    "    return list(tokens.values())\n",
    "\n",
    "tokens_october = fetch_october_2023_tokens_from_pools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0306997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"214ad8fea4f34456c35952f32963b967\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "SUBGRAPH_URL = \"https://gateway.thegraph.com/api/subgraphs/id/5zvR82QoaXYFyDEKLZ9t6v9adgnptxYpKpSbxtgVENFV\"\n",
    "\n",
    "# timestamps\n",
    "START_TS = 1709164800  # mar 1, 2024\n",
    "END_TS = 1711843200    # mar 31, 2024\n",
    "\n",
    "def run_query(query):\n",
    "    response = requests.post(SUBGRAPH_URL, json={\"query\": query}, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        resp_json = response.json()\n",
    "        if \"errors\" in resp_json:\n",
    "            raise Exception(f\"GraphQL errors: {resp_json['errors']}\")\n",
    "        return resp_json[\"data\"]\n",
    "    else:\n",
    "        raise Exception(f\"Query failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "def fetch_pools_created_october():\n",
    "    pools = []\n",
    "    skip = 0\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "          pools(\n",
    "            first: {batch_size},\n",
    "            skip: {skip},\n",
    "            orderBy: createdAtTimestamp,\n",
    "            orderDirection: asc,\n",
    "            where: {{\n",
    "              createdAtTimestamp_gte: {START_TS},\n",
    "              createdAtTimestamp_lt: {END_TS}\n",
    "            }}\n",
    "          ) {{\n",
    "            id\n",
    "            createdAtTimestamp\n",
    "            token0 {{\n",
    "              id\n",
    "              symbol\n",
    "              name\n",
    "              decimals\n",
    "            }}\n",
    "            token1 {{\n",
    "              id\n",
    "              symbol\n",
    "              name\n",
    "              decimals\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        data = run_query(query)\n",
    "        batch = data[\"pools\"]\n",
    "        pools.extend(batch)\n",
    "        print(f\"Fetched {len(batch)} pools, total so far: {len(pools)}\")\n",
    "        if len(batch) < batch_size:\n",
    "            break\n",
    "        skip += batch_size\n",
    "        time.sleep(0.3)  # be gentle to API\n",
    "\n",
    "    return pools\n",
    "\n",
    "def extract_unique_tokens_from_pools(pools):\n",
    "    tokens = {}\n",
    "    for pool in pools:\n",
    "        for token_key in [\"token0\", \"token1\"]:\n",
    "            token = pool[token_key]\n",
    "            tokens[token[\"id\"]] = {\n",
    "                \"symbol\": token[\"symbol\"],\n",
    "                \"name\": token[\"name\"],\n",
    "                \"decimals\": int(token[\"decimals\"]),\n",
    "                \"address\": token[\"id\"],\n",
    "            }\n",
    "    return tokens\n",
    "\n",
    "def fetch_pool_day_data(pool_ids):\n",
    "    pool_volumes = defaultdict(float)\n",
    "    batch_size = 50\n",
    "\n",
    "    # Fetch daily volumes for each pool for October\n",
    "    for i in range(0, len(pool_ids), batch_size):\n",
    "        batch_ids = pool_ids[i : i + batch_size]\n",
    "        ids_string = ','.join(f'\"{pid}\"' for pid in batch_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "          poolDayDatas(\n",
    "            where: {{\n",
    "              pool_in: [{ids_string}],\n",
    "              date_gte: {START_TS},\n",
    "              date_lt: {END_TS}\n",
    "            }},\n",
    "            first: 1000\n",
    "          ) {{\n",
    "            pool {{\n",
    "              id\n",
    "              token0 {{\n",
    "                id\n",
    "                symbol\n",
    "              }}\n",
    "              token1 {{\n",
    "                id\n",
    "                symbol\n",
    "              }}\n",
    "            }}\n",
    "            volumeUSD\n",
    "            date\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        data = run_query(query)\n",
    "        day_data = data[\"poolDayDatas\"]\n",
    "\n",
    "        for record in day_data:\n",
    "            pool = record[\"pool\"]\n",
    "            vol = float(record[\"volumeUSD\"])\n",
    "            # Assign half volume to token0 and half to token1 (simplification)\n",
    "            pool_volumes[pool[\"token0\"][\"id\"]] += vol / 2\n",
    "            pool_volumes[pool[\"token1\"][\"id\"]] += vol / 2\n",
    "\n",
    "        print(f\"Processed batch {i} to {i+batch_size}, total poolDayData: {len(day_data)}\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return pool_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513bed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Fetching pools created in mar 2024...\")\n",
    "    pools = fetch_pools_created_october()\n",
    "    print(f\"Total pools fetched: {len(pools)}\")\n",
    "\n",
    "    print(\"Extracting unique tokens from pools...\")\n",
    "    tokens = extract_unique_tokens_from_pools(pools)\n",
    "    print(f\"Total unique tokens found: {len(tokens)}\")\n",
    "\n",
    "    pool_ids = [pool[\"id\"] for pool in pools]\n",
    "    print(f\"Fetching poolDayData for {len(pool_ids)} pools...\")\n",
    "\n",
    "    volumes = fetch_pool_day_data(pool_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pools created in mar 2024...\n",
      "Fetched 568 pools, total so far: 568\n",
      "Total pools fetched: 568\n",
      "Extracting unique tokens from pools...\n",
      "Total unique tokens found: 542\n",
      "Fetching poolDayData for 568 pools...\n",
      "Processed batch 0 to 50, total poolDayData: 579\n",
      "Processed batch 50 to 100, total poolDayData: 414\n",
      "Processed batch 100 to 150, total poolDayData: 450\n",
      "Processed batch 150 to 200, total poolDayData: 405\n",
      "Processed batch 200 to 250, total poolDayData: 305\n",
      "Processed batch 250 to 300, total poolDayData: 254\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Fetching pools created in mar 2024...\")\n",
    "    pools = fetch_pools_created_october()\n",
    "    print(f\"Total pools fetched: {len(pools)}\")\n",
    "\n",
    "    print(\"Extracting unique tokens from pools...\")\n",
    "    tokens = extract_unique_tokens_from_pools(pools)\n",
    "    print(f\"Total unique tokens found: {len(tokens)}\")\n",
    "\n",
    "    pool_ids = [pool[\"id\"] for pool in pools]\n",
    "    print(f\"Fetching poolDayData for {len(pool_ids)} pools...\")\n",
    "\n",
    "    volumes = fetch_pool_day_data(pool_ids)\n",
    "    print(f\"Calculated monthly volumes for {len(volumes)} tokens.\")\n",
    "\n",
    "    # Sort tokens by volume descending\n",
    "    sorted_tokens = sorted(volumes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 5 tokens by volume in mar 2024:\")\n",
    "    for token_id, vol in sorted_tokens[:5]:\n",
    "        symbol = tokens[token_id][\"symbol\"] if token_id in tokens else \"UNKNOWN\"\n",
    "        print(f\"{symbol} ({token_id}): ${vol:,.2f}\")\n",
    "\n",
    "    # Now slice tokens ranked 100 to 199 (0-based indexing)\n",
    "    subset_tokens = sorted_tokens[100:200]\n",
    "\n",
    "    print(\"\\nTop 5 tokens by volume ranked 100-200:\")\n",
    "    for token_id, vol in subset_tokens[:5]:\n",
    "        symbol = tokens[token_id][\"symbol\"] if token_id in tokens else \"UNKNOWN\"\n",
    "        print(f\"{symbol} ({token_id}): ${vol:,.2f}\")\n",
    "\n",
    "    # Now slice tokens ranked 50 to 99 (0-based indexing)\n",
    "    subset_tokens = sorted_tokens[50:100]\n",
    "\n",
    "    print(\"\\nTop 5 tokens by volume ranked 50-100:\")\n",
    "    for token_id, vol in subset_tokens[:5]:\n",
    "        symbol = tokens[token_id][\"symbol\"] if token_id in tokens else \"UNKNOWN\"\n",
    "        print(f\"{symbol} ({token_id}): ${vol:,.2f}\")\n",
    "        \n",
    "        \n",
    "    # Sort tokens by volume descending\n",
    "    sorted_tokens = sorted(volumes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 20 tokens by volume in mar 2024:\")\n",
    "    for token_id, vol in sorted_tokens[:20]:\n",
    "        symbol = tokens[token_id][\"symbol\"] if token_id in tokens else \"UNKNOWN\"\n",
    "        print(f\"{symbol} ({token_id}): ${vol:,.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f1243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
